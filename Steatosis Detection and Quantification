{"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7dc84e3339c94027b7d1f84cc4c4fe63":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d0a55acabdd24da3a3d3d5a830edbe05","IPY_MODEL_865d170b905b4b74aee3ad3e100dcdce","IPY_MODEL_d1dd7346e755441c9705b377885b954c"],"layout":"IPY_MODEL_96b5d66e1db24b9fb840c9fbbfdbf965"}},"d0a55acabdd24da3a3d3d5a830edbe05":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be16f79144aa4a7ba82ee05617a8be48","placeholder":"​","style":"IPY_MODEL_c1caba883c434dd899c43eea54a7f57b","value":"config.json: 100%"}},"865d170b905b4b74aee3ad3e100dcdce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e81e74c1f87489c96337c525405c831","max":502,"min":0,"orientation":"horizontal","style":"IPY_MODEL_315913a42df6469280335c330e17a0c9","value":502}},"d1dd7346e755441c9705b377885b954c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_533bb8e554c54e2c9c73a0bc6c30860d","placeholder":"​","style":"IPY_MODEL_1571a0433a034001b83206ce8df98768","value":" 502/502 [00:00&lt;00:00, 14.6kB/s]"}},"96b5d66e1db24b9fb840c9fbbfdbf965":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be16f79144aa4a7ba82ee05617a8be48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1caba883c434dd899c43eea54a7f57b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e81e74c1f87489c96337c525405c831":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"315913a42df6469280335c330e17a0c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"533bb8e554c54e2c9c73a0bc6c30860d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1571a0433a034001b83206ce8df98768":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3585cd9309c1457d810564c7050853e3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_368d06fa2c9d4f5daeec91e1a7554126","IPY_MODEL_d4c658cc73e9431091a1201829717e91","IPY_MODEL_e4cb2a3321f1476d8198d93d1e0ab0bd"],"layout":"IPY_MODEL_1d6fc7507e2c46468562a017d973d44e"}},"368d06fa2c9d4f5daeec91e1a7554126":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0d8a5bf68574337ab17d3474e185277","placeholder":"​","style":"IPY_MODEL_88d4565b5c2f4f8ab213606101cc1120","value":"model.safetensors: 100%"}},"d4c658cc73e9431091a1201829717e91":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7d0c18151e44e1a993dde05a6d42e5c","max":345579424,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4574f014cfd74a4280865b39657bebe9","value":345579424}},"e4cb2a3321f1476d8198d93d1e0ab0bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46b1fc190d874d2b9ccfe6f13a790549","placeholder":"​","style":"IPY_MODEL_8658d70aeab048669517f4188dc4a1aa","value":" 346M/346M [00:02&lt;00:00, 157MB/s]"}},"1d6fc7507e2c46468562a017d973d44e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0d8a5bf68574337ab17d3474e185277":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88d4565b5c2f4f8ab213606101cc1120":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7d0c18151e44e1a993dde05a6d42e5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4574f014cfd74a4280865b39657bebe9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"46b1fc190d874d2b9ccfe6f13a790549":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8658d70aeab048669517f4188dc4a1aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"420c407f83da441b85fa90c18adfc57b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_89bbfe0bd3c14fab911b8d09c5228c70","IPY_MODEL_b5e9ba8da013467eaef5f00e1828e43e","IPY_MODEL_d836b6f0a16a4089853bd224ca4a24a7"],"layout":"IPY_MODEL_2ae3876b193442e687573a8ba6d8ab25"}},"89bbfe0bd3c14fab911b8d09c5228c70":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c72821599960455e9e05bef05482c2e3","placeholder":"​","style":"IPY_MODEL_4faac1f8712049ac9ad1b099d0ce8d30","value":"preprocessor_config.json: 100%"}},"b5e9ba8da013467eaef5f00e1828e43e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_892a66c9c73146949af483ef38018d9c","max":160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_04df3d540f6b47d6bc14136663e21848","value":160}},"d836b6f0a16a4089853bd224ca4a24a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21f8a2ad5efa43bdaacaabbb5890d9f5","placeholder":"​","style":"IPY_MODEL_b2dbd364d152439bbe915bbf77628acd","value":" 160/160 [00:00&lt;00:00, 8.66kB/s]"}},"2ae3876b193442e687573a8ba6d8ab25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c72821599960455e9e05bef05482c2e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4faac1f8712049ac9ad1b099d0ce8d30":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"892a66c9c73146949af483ef38018d9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04df3d540f6b47d6bc14136663e21848":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"21f8a2ad5efa43bdaacaabbb5890d9f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2dbd364d152439bbe915bbf77628acd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom sklearn.cluster import KMeans\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport shutil","metadata":{"id":"_qrkk_BOKosT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip '/content/drive/MyDrive/major_steto/Steatosis_Dataset_major.zip' -d '/content/drive/MyDrive/Steatosis_Dataset'","metadata":{"id":"DolNZoJUM9Y6","outputId":"e658b575-8f62-4128-8476-0a021ed6e67e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset='/content/drive/MyDrive/Steatosis_Dataset/Steatosis_Dataset'\nfor img in os.listdir(dataset):\n  print(img)","metadata":{"id":"ac9coZxiM1o5","outputId":"a55a725e-79b9-4e3d-8148-a342c56ce43c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_feature(dataset):\n    model = InceptionV3(weights='imagenet', include_top=False)\n    features = [];\n    img_name = [];\n    for i in tqdm(dataset):\n        fname='/content/drive/MyDrive/Steatosis_Dataset/Steatosis_Dataset'+'/'+i\n        img=image.load_img(fname,target_size=(224,224))\n        x = img_to_array(img)\n        x=np.expand_dims(x,axis=0)\n        x=preprocess_input(x)\n        feat=model.predict(x)\n        feat=feat.flatten()\n        features.append(feat)\n        img_name.append(i)\n    return features,img_name","metadata":{"id":"sF8oJuQCM-Rx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nImage.MAX_IMAGE_PIXELS = None # Set to the actual imagesize","metadata":{"id":"qEjQ5wq6NEpL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_path=os.listdir('/content/drive/MyDrive/Steatosis_Dataset/Steatosis_Dataset')\nimg_features,img_names=image_feature(img_path)","metadata":{"id":"iwzQW7ouNQwY","outputId":"901244b9-5d0a-427c-bd45-071556821d7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k = 2\nclusters = KMeans(k, random_state = 42)\nclusters.fit(img_features)","metadata":{"id":"N3gFoRavNUz8","outputId":"a0d4c36a-39bd-4c07-c921-6b51bb43bc4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_cluster = pd.DataFrame(img_names,columns=['image'])\nimage_cluster[\"clusterid\"] = clusters.labels_\nimage_cluster # 0 denotes cat and 1 denotes dog","metadata":{"id":"OQrhqvDdudn5","outputId":"2625d7ae-4872-464b-ed62-8a3a18709be7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimage_cluster['clusterid'].plot(kind='line', figsize=(8, 4), title='clusterid')\nplt.gca().spines[['top', 'right']].set_visible(False)","metadata":{"id":"PxHhj-ciuvmb","outputId":"5eb16ea9-6e7d-4829-ad90-b71ef23d3394"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimage_cluster['clusterid'].plot(kind='hist', bins=20, title='clusterid')\nplt.gca().spines[['top', 'right',]].set_visible(False)","metadata":{"id":"OXUVG9vauuhZ","outputId":"c5ae9919-c50e-41b4-aa3f-8f365707ceae"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Made folder to seperate images\nos.mkdir('0')\nos.mkdir('1')\n# Images will be seperated according to cluster they belong\nfor i in range(len(image_cluster)):\n    if image_cluster['clusterid'][i]==0:\n        shutil.move(os.path.join('/content/drive/MyDrive/Steatosis_Dataset/Steatosis_Dataset', image_cluster['image'][i]), '0')\n    else:\n        shutil.move(os.path.join('/content/drive/MyDrive/Steatosis_Dataset/Steatosis_Dataset', image_cluster['image'][i]), '1')","metadata":{"id":"0qPvjXkuwzX0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install liverquant\n","metadata":{"id":"P1EJrnFbw0wx","outputId":"3f2e4929-4924-4a0b-f36b-a9c9c8d9fce4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"-NkCqHzks7rz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport os\n\ninput_folder = '/content/0'\noutput_folder = '/content/liver_tiff'\n\nos.makedirs(output_folder, exist_ok=True)\n\nconverted_images = []\n\nfor filename in os.listdir(input_folder):\n    if filename.endswith('.jpg'):\n        input_path = os.path.join(input_folder, filename)\n        output_path = os.path.join(output_folder, os.path.splitext(filename)[0] + '.tiff')\n\n        with Image.open(input_path) as img:\n            img.save(output_path, format='TIFF')\n            converted_images.append(output_path)\n\n# Print the paths of the converted TIFF images\nfor tiff_path in converted_images:\n    print(tiff_path)\n\n","metadata":{"id":"DeaZ8QYazC_8","outputId":"13cb5d72-7a84-4ed9-98f4-80b815a29512","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n\n# Open the image file using Pillow\nimg = Image.open('/content/liver_tiff/132_2.tiff')\n\n# Display the image\nplt.imshow(img)\nplt.axis('off')  # Turn off axis numbers\nplt.show()\n\n# Get some information about the image\nprint(f\"Image format: {img.format}\")\nprint(f\"Image size: {img.size}\")","metadata":{"id":"w2ILm-3T1YWg","outputId":"8c9540cc-1c0b-4d4d-ad08-2a0999b5fce1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\njpg_image=Image.open('/content/0/133_4.jpg')\ntif_image=Image.open('/content/liver_tiff/133_4.tiff')\n\nfig,axs=plt.subplots(1,2,figsize=(14,6))\naxs[0].imshow(jpg_image)\naxs[0].axis('off')\naxs[0].set_title('JPG Image')\n\naxs[1].imshow(tif_image)\naxs[1].axis('off')\naxs[1].set_title('TIFF Image')\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\n\n","metadata":{"id":"ueb_Jnzt1mYo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2 as cv\nfrom liverquant import detect_fat_globules\n\n# read sample image tile\nimg = cv.imread('/content/liver_tiff/132_4.tiff')\nimg = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n\n# Detect globules\nmask = detect_fat_globules(img, resolution=0.7878)\n\n# Tag globules with green color\nimg[mask == 255, :] = [5, 255, 5]\n\n# write the tagged image\nsuccess=coloured_image=cv.imwrite('/content/liver_tiff/132_4_mod.tiff', cv.cvtColor(img, cv.COLOR_BGR2RGB))\nprint(success)\nprint(type(coloured_image))\n# plt.imshow(coloured_image)\n# plt.title('Modified Image')\n# plt.axis('off')  # Hide axes\n# plt.show()","metadata":{"id":"rI_NF_mTQzlk","outputId":"47d796ed-066a-4e47-ffb5-84818eb849b4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tiffslide","metadata":{"id":"f2okVhQjupFu","outputId":"d67cf027-8713-484e-fbdf-462cd82b53e9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2 as cv\nimport numpy as np\nfrom liverquant import detect_fat_globules\nfrom tiffslide import TiffSlide  # Assuming this is the library for reading whole slide images\n\n# Read whole slide image\nslide_path = '/content/liver_tiff/133_4.tiff'\nslide = TiffSlide(slide_path)\n\n# Get the dimensions of the whole slide image\nslide_width, slide_height = slide.dimensions\n\n# Initialize an empty mask for storing fat globules\nmask = np.zeros((slide_height, slide_width), dtype=np.uint8)\n\n# Define the size of the tiles (e.g., 512x512 pixels)\ntile_width = 5000\ntile_height = 3000\n\nlevel = 0\n\n# Process the whole slide image\nfor x in range(0, slide_width, tile_width):\n  if x + tile_width > slide_width:\n        x = slide_width - tile_width\n  for y in range(0, slide_height, tile_height):\n    if y + tile_height > slide_height:\n            y = slide_height - tile_height\n\n        # Read a tile\n    tile = slide.read_region((x, y), level, (tile_width, tile_height))\n\n        # Convert tile to RGB (assuming it's not already in RGB format)\n    tile = cv.cvtColor(np.array(tile), cv.COLOR_BGR2RGB)\n\n        # Detect fat globules in the tile\n    tile_mask = detect_fat_globules(tile, resolution=0.1122)\n        # Option 1: Resize the tile_mask to match the mask section\n    mask[y:y+tile_height, x:x+tile_width] = tile_mask\n\n# Option 2: Crop the mask section to match the tile_mask\n\n        # Update the mask with the detected fat globules\n        # mask[y:y+tile_height, x:x+tile_width] = tile_mask\n\n# Tag globules with green color in the original whole slide image\nimg = np.array(slide.read_region((0, 0), level, (slide_width, slide_height)))\nimg[mask == 255, :] = [5, 255, 5]\n\n# Write the modified image\nsuccess = cv.imwrite('/content/liver_tiff/133_4_mod.tiff', cv.cvtColor(img, cv.COLOR_BGR2RGB))\nprint(success)\n","metadata":{"id":"EPRxWqMVcljP","outputId":"879bae76-f525-49ab-b0b3-8988e24a42cd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Read the modified image\nmodified_img = mpimg.imread('/content/liver_tiff/133_4_mod.tiff')\n\n# Display the image\nplt.imshow(modified_img)\nplt.axis('off')  # Hide axes\nplt.show()\n","metadata":{"id":"hQh_r0S0yNdj","outputId":"ba8db15b-7399-45fa-c848-cc098e0e210b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Define paths to training and testing folders\ntrain_folder = '/path/to/training/folder'\ntest_folder = '/path/to/testing/folder'\n\n# Initialize lists to store processed images and labels\ntrain_images = []\ntrain_labels = []\ntest_images = []\ntest_labels = []\n\n# Iterate over the images in the training folder\nfor filename in os.listdir(train_folder):\n    if filename.endswith(\".jpg\"):\n        img_path = os.path.join(train_folder, filename)\n        img = Image.open(img_path)\n        img_array = np.array(img)\n        img_array = tf.expand_dims(img_array, 0)\n        train_images.append(img_array)\n        # Assuming you have a way to determine the label for each image\n        train_labels.append(get_label_from_filename(filename))\n\n# Iterate over the images in the testing folder\nfor filename in os.listdir(test_folder):\n    if filename.endswith(\".jpg\"):\n        img_path = os.path.join(test_folder, filename)\n        img = Image.open(img_path)\n        img_array = np.array(img)\n        img_array = tf.expand_dims(img_array, 0)\n        test_images.append(img_array)\n        # Assuming you have a way to determine the label for each image\n        test_labels.append(get_label_from_filename(filename))\n\n# Convert the lists to NumPy arrays\ntrain_images = np.array(train_images)\ntrain_labels = np.array(train_labels)\ntest_images = np.array(test_images)\ntest_labels = np.array(test_labels)\n\n# Use the processed images and labels for further processing or training\n","metadata":{"id":"i_uOUm1ruKRy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install skimage","metadata":{"id":"JoJuAXSwNFuD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom PIL import Image\n\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\n\nfrom transformers import ViTForImageClassification, ViTImageProcessor\n\nmodel_name_or_path = 'google/vit-base-patch16-224-in21k'\nmodel = ViTForImageClassification.from_pretrained(model_name_or_path)\nprocessor = ViTImageProcessor.from_pretrained(model_name_or_path)\n# Step 1: Preprocess Images\nimport os\n\n# Path to the folder containing images\nfolder_path = '/content/0'\n\n# Get a list of all files in the folder\nimage_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n\n\n# image_file = image_files[0]\n# image_path = os.path.join(folder_path,image_file)\n\n\n# Process each image in the folder\nfor image_file in image_files:\n    # Construct the full path to the image file\n    image_path = os.path.join(folder_path, image_file)\n\n    # Load and preprocess the image\n    img = Image.open(image_path)\n    img_array = np.array(img)\n    img_array = tf.expand_dims(img_array, 0)  # Create a batch\n\n    # Use Vision Transformer\n    features = ViTFeatureExtractor(model_name_or_path)(images=img_array, return_tensors=\"pt\")\n    outputs = model(**features)\n    # print(type(outputs))\n    # last_hidden_state = outputs.pooler_output\n    # last_hidden_state = outputs.last_hidden_state\n\n    patch_embeddings =features['pixel_values']\n\n    num_patches = patch_embeddings.shape[1]\n    sqrt_num_patch = int(np.ceil(np.sqrt(num_patches)))\n\n    plt.figure(figsize = (12,12))\n    for i in range(num_patches):\n        plt.subplot(sqrt_num_patch, sqrt_num_patch, i + 1)\n        patch = patch_embeddings[0,i].numpy()\n        plt.imshow(patch)\n        plt.axis('off')\n        plt.title(f'Patch {i}')\n\n    plt.suptitle(f'Patches Extracted by Vision Transformer for Image: {image_file}', y=0.92)\n    plt.tight_layout()\n    plt.show()\n\n    total_area = patch_embeddings.shape[1]  # Total number of patches\n    steatosis_area = patch_embeddings.sum()\n    print(steatosis_area) # Sum of patch embeddings (higher values indicate more steatosis)\n    steatosis_proportionate_area = (steatosis_area / total_area)/1000\n\n    print(f\"Steatosis Proportionate Area : {steatosis_proportionate_area:.2f}\")\n\n\n\n\n# layer_features = outputs['hidden_states'][-2]\n\n# plt.figure(figsize = (6,6))\n# plt.show(img_array[0])\n# plt.axis('off')\n# plt.title('Original Image')\n\n# Display the patches extracted by the Vision Transformer\n#     num_patches = last_hidden_state.shape[1]\n# # patch_size = int(np.sqrt(last_hidden_state.shape[2]))\n#     sqrt_num_patches = int(np.ceil(np.sqrt(num_patches)))\n\n# plt.figure(figsize=(12, 12))\n# for i in range(num_patches):\n#     plt.subplot(sqrt_num_patches, sqrt_num_patches, i + 1)\n#     patch = last_hidden_state[0,i].reshape(patch_size, patch_size).detach().cpu().numpy()\n#     plt.imshow(patch, cmap='gray')\n#     plt.axis('off')\n#     plt.title(f'Patch {i}')\n\n# plt.suptitle('Patches Extracted by Vision Transformer', y=0.92)\n# plt.tight_layout()\n# plt.show()\n#     # Process the outputs as needed\n    # For example, extract steatosis regions and measure their area proportion\n\n","metadata":{"id":"48fS47Gu06AP","outputId":"2179a23c-7886-44ed-ed52-e1c3702e5547"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_path = '/content/0/132_2.jpg'\nimg = Image.open(img_path)\n\nimg_array = np.array(img)\nimg_array = tf.expand_dims(img_array, 0)  # Create a batch\n\n# Detect steatosis in the image\n\n# def preprocess_image(image_path):\n#     image = cv2.imread(image_path)\n#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n#     image = cv2.resize(image, (224, 224))  # Resize to match model input size\n#     image = np.array(image) / 255.0  # Normalize pixel values to [0, 1]\n#     return image\n\n# Step 2: Use Vision Transformer\nfeatures = ViTFeatureExtractor(model_name_or_path)(images=img_array, return_tensors=\"pt\")\n# features = feature_extractor(images=img_array, return_tensors=\"pt\")\noutputs = model(**features)\n","metadata":{"id":"y5NZvBTbYcc_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\n# Path to the folder containing images\nfolder_path = '/content/zero/'\n\n# Iterate over all files in the folder\nfor file_name in os.listdir(folder_path):\n    # Check if the file is an image\n    if file_name.endswith(('.jpg', '.jpeg', '.png', '.tif', '.tiff')):\n        # Construct the full path to the image file\n        img_path = os.path.join(folder_path, file_name)\n        img = Image.open(img_path)\n\n        img_array = np.array(img)\n        img_array = tf.expand_dims(img_array, 0)  # Create a batch\n\n        # Use Vision Transformer\n        features = ViTFeatureExtractor(model_name_or_path)(images=img_array, return_tensors=\"pt\")\n        outputs = model(**features)\n\n        # Process the outputs as needed\n        # For example, print the predictions for each image\n        print(f\"Predictions for {file_name}: {outputs}\")\n","metadata":{"id":"SuyLrVS8VTsp","outputId":"d65333b9-080e-454b-ced3-1e4b1ce26b4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install explainerdashboard","metadata":{"id":"-ZYWqLxcjPo3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom explainerdashboard import ClassifierExplainer, ExplainerDashboard\n\n# from skimage.feature import graycomatrix, graycoprops\n\n\n# from explainerdashboard import ClassifierExplainer, ExplainerDashboard\n# from transformers import ViTForImageClassification\n\n # Define the model\n# model_name_or_path = 'google/vit-base-patch16-224-in21k'\n# model = ViTForImageClassification.from_pretrained(model_name_or_path)\n\n# Path to the folder containing images\nfolder_path = '/content/0'\nimage_data = np.array([cv2.imread(os.path.join(folder_path, filename), cv2.IMREAD_GRAYSCALE) for filename in os.listdir(folder_path)])\n# Example minimum area threshold (adjust as needed)\nmin_area_threshold = 100\n\n# Initialize lists to store feature importance values\n# pixel_intensity_importance = []\n# texture_importance = []\n# shape_importance = []\n\n# grades = {\n#     'mild': (0.05, 0.33),  # Mild steatosis: 5-33% steatosis proportionate area\n#     'moderate': (0.33, 0.66),  # Moderate steatosis: 33-66% steatosis proportionate area\n#     'severe': (0.66, 1.0)  # Severe steatosis: 66-100% steatosis proportionate area\n# }\n# explainer = ClassifierExplainer(model,image_data,image_data,labels = ['mild','moderate','severe'])\n\n# # Initialize dictionaries to store images for each grade\n# mild_images = []\n# moderate_images = []\n# severe_images = []\n\nfor filename in os.listdir(folder_path):\n    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n        # Load the image\n        img_path = os.path.join(folder_path, filename)\n        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n\n        # Apply OTSU's thresholding\n        _, thresholded_image = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n\n        # Connected Component Analysis (CCA)\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(thresholded_image, connectivity=8)\n\n        # Calculate Steatosis Proportionate Area\n        total_area = thresholded_image.shape[0] * thresholded_image.shape[1]\n        steatosis_proportionate_area = sum(stats[label, cv2.CC_STAT_AREA] for label in range(1, num_labels)) / total_area\n        print(f\"Steatosis Proportionate Area for {filename}: {steatosis_proportionate_area}\")\n\n  explainer = ClassifierExplainer(model, image_data, labels=['mild', 'moderate', 'severe'])\n  ExplainerDashboard(explainer).run()\n\n\n\n\n    #     # Compute pixel intensity feature importance\n    #     pixel_intensity_importance.append(np.mean(img))\n\n    #     # Compute texture feature importance (using GLCM)\n    #     glcm = graycomatrix(img, [1], [0], symmetric=True, normed=True)\n    #     texture_importance.append(graycoprops(glcm, 'contrast')[0, 0])\n\n    #     # Compute shape feature importance\n    #     contours, _ = cv2.findContours(thresholded_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    #     shape_importance.append(len(contours))\n\n#     # else:\n#     #     continue\n\n#         grade = None\n#         for g, (lower, upper) in grades.items():\n#             if lower <= steatosis_proportionate_area <= upper:\n#                 grade = g\n#                 break\n\n#         # Store the image in the corresponding grade dictionary\n#         if grade == 'mild':\n#             mild_images.append(img)\n#         elif grade == 'moderate':\n#             moderate_images.append(img)\n#         elif grade == 'severe':\n#             severe_images.append(img)\n\n#     else:\n#         continue\n\n# # Display ExplainerDashboard\n# db = ExplainerDashboard(explainer, title=\"Steatosis Grading Dashboard\")\n# db.run()\n\n# # Create a DataFrame from the computed feature importance values\n# import pandas as pd\n# data = {'Pixel Intensity Importance': pixel_intensity_importance,\n#         'Texture Importance': texture_importance,\n#         'Shape Importance': shape_importance}\n# df = pd.DataFrame(data)\n\n# # Create a ClassifierExplainer\n# explainer = ClassifierExplainer(df, np.random.randint(0, 2, len(df)), labels=['Steatosis', 'Non-Steatosis'])\n\n# # Create an ExplainerDashboard\n# ExplainerDashboard(explainer).run()\n","metadata":{"id":"dUHZ-mp4jHjK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom google.colab.patches import cv2_imshow\n\n# Example threshold value (adjust as needed)\nthreshold_value = 0.5\n\n# Example model outputs (replace with actual model outputs)\nmodel_outputs = np.random.rand(224, 224)  # Assuming model output shape is (224, 224)\n\n# Thresholding\nthresholded_image = (model_outputs > threshold_value).astype(np.uint8) * 255\n\n# Connected Component Analysis (CCA)\nnum_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(thresholded_image, connectivity=8)\n\n# Region Filtering\nmin_area_threshold = 100  # Example minimum area threshold (adjust as needed)\nfiltered_labels = [label for label in range(1, num_labels) if stats[label, cv2.CC_STAT_AREA] > min_area_threshold]\n\n# Calculate Steatosis Proportionate Area\ntotal_area = thresholded_image.shape[0] * thresholded_image.shape[1]\nsteatosis_proportionate_area = sum(stats[label, cv2.CC_STAT_AREA] for label in filtered_labels) / total_area\n\n# Extract ROIs (optional)\nrois = [thresholded_image == label for label in filtered_labels]\n\n# Display the thresholded image (for visualization)\nplt.imshow(thresholded_image,cmap='gray')\nplt.axis('off')\nplt.show()\n# cv2.imshow('Thresholded Image', thresholded_image)\n# cv2.waitKey(0)\n# cv2.destroyAllWindows()\n\n# Display the filtered ROIs (for visualization)\nfor roi in rois:\n    plt.imshow( roi.astype(np.uint8) * 255,cmap='gray')\n    plt.axis('off')\n    plt.show()\n\n# Print the steatosis proportionate area\nprint(f\"Steatosis Proportionate Area: {steatosis_proportionate_area}\")\n","metadata":{"id":"-_Ak8Ufx08kM","outputId":"28c2f0b3-fba6-4a08-9e26-62c8e85b7217"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Nbf4HGH0ZmGI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport dash_core_components as dcc\nimport dash_html_components as html\nimport plotly.express as px\nfrom explainerdashboard import ClassifierExplainer, ExplainerDashboard\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\nfrom PIL import Image\nimport types\n\n\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\n\nfrom transformers import ViTForImageClassification, ViTImageProcessor\n\nmodel_name_or_path = 'google/vit-base-patch16-224-in21k'\nmodel = ViTForImageClassification.from_pretrained(model_name_or_path)\nprocessor = ViTImageProcessor.from_pretrained(model_name_or_path)\n# Step 1: Preprocess Images\nimport os\ndef predict_proba(self, X):\n    outputs = self(X)\n    probabilities = tf.nn.softmax(outputs.logits, axis=-1)\n    return probabilities.numpy()\n\n# Monkey patch the predict_proba method to the ViTForImageClassification model\nmodel.predict_proba = types.MethodType(predict_proba, model)\n\n# Path to the folder containing images\nfolder_path = '/content/0'\n\nthreshold_mild = 0.33\nthreshold_moderate = 0.66\n\n# Get a list of all files in the folder\nimage_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n\nspa_values =[]\nfor image_file in image_files:\n    # Construct the full path to the image file\n    image_path = os.path.join(folder_path, image_file)\n\n    # Load and preprocess the image\n    img = Image.open(image_path)\n    img_array = np.array(img)\n    img_array = tf.expand_dims(img_array, 0)  # Create a batch\n\n    # Use Vision Transformer\n    features = ViTFeatureExtractor(model_name_or_path)(images=img_array, return_tensors=\"pt\")\n    outputs = model(**features)\n    # print(type(outputs))\n    # last_hidden_state = outputs.pooler_output\n    # last_hidden_state = outputs.last_hidden_state\n\n    patch_embeddings =features['pixel_values']\n\n    num_patches = patch_embeddings.shape[1]\n    sqrt_num_patch = int(np.ceil(np.sqrt(num_patches)))\n\n    # plt.figure(figsize = (12,12))\n    # for i in range(num_patches):\n    #     plt.subplot(sqrt_num_patch, sqrt_num_patch, i + 1)\n    #     patch = patch_embeddings[0,i].numpy()\n    #     plt.imshow(patch)\n    #     plt.axis('off')\n    #     plt.title(f'Patch {i}')\n\n    # plt.suptitle(f'Patches Extracted by Vision Transformer for Image: {image_file}', y=0.92)\n    # plt.tight_layout()\n    # plt.show()\n\n    total_area = patch_embeddings.shape[1]  # Total number of patches\n    steatosis_area = patch_embeddings.sum()\n    # print(steatosis_area) # Sum of patch embeddings (higher values indicate more steatosis)\n    steatosis_proportionate_area = (steatosis_area / total_area)/100000\n\n    # print(f\"Steatosis Proportionate Area : {steatosis_proportionate_area:.2f}\")\n# Assuming you have already calculated the Steatosis Proportionate Area (SPA)\n    # spa_value = steatosis_proportionate_area\n    spa_values.append(steatosis_proportionate_area)\n\n     # Replace with your actual SPA value\n    spa_values_df = pd.DataFrame({'spa_value':spa_values})\n    print(spa_values_df.columns)\n    spa_values_df['category'] = np.where(spa_values_df['spa_value'] < threshold_mild, 'Mild',\n                                      np.where(spa_values_df['spa_value'] < threshold_moderate, 'Moderate', 'Extreme'))\n\n# Create a histogram of SPA values\nfig = px.histogram(spa_values_df, x='spa_value', nbins=20, labels={'spa_value': 'Steatosis Proportionate Area'})\n\n# Create a custom component\ncustom_component = html.Div([\n    html.H3(\"Distribution of Steatosis Proportionate Area\"),\n    dcc.Graph(figure=fig),\n])\n\n# Create an Explainer instance\nexplainer = ClassifierExplainer(model, spa_values_df, labels=spa_values_df['category'])\nexplainer.run()\n\n# Run the Explainer Dashboard with the custom component\n# ExplainerDashboard(explainer, custom_component=custom_component).run()\n\n\n# Define thresholds for mild, moderate, and extreme\n\n\n# Categorize SPA value\n#     category = np.where(spa_value < threshold_mild, 'Mild',\n#                     np.where(spa_value < threshold_moderate, 'Moderate', 'Extreme'))\n\n# # Create a histogram of SPA values\n#     fig = px.histogram([spa_value], nbins=20, labels={'value': 'Steatosis Proportionate Area'})\n\n# # Create a custom component\n# custom_component = html.Div([\n#     html.H3(\"Distribution of Steatosis Proportionate Area\"),\n#     dcc.Graph(figure=fig),\n# ])\n\n# spa_value = spa_value.numpy()\n# spa_value_df['category'] = pd.DataFrame({'spa_value': spa_value})\n# # Create an Explainer instance (replace with your actual model and data)\n# explainer = ClassifierExplainer(model, spa_value_df)\n\n# # Run the Explainer Dashboard with the custom component\n# ExplainerDashboard(explainer, custom_component=custom_component).run()\n\n\n\n\n# image_file = image_files[0]\n# image_path = os.path.join(folder_path,image_file)\n\n\n# Process each image in the folder\n\n\n\n\n\n# layer_features = outputs['hidden_states'][-2]\n\n# plt.figure(figsize = (6,6))\n# plt.show(img_array[0])\n# plt.axis('off')\n# plt.title('Original Image')\n\n# Display the patches extracted by the Vision Transformer\n#     num_patches = last_hidden_state.shape[1]\n# # patch_size = int(np.sqrt(last_hidden_state.shape[2]))\n#     sqrt_num_patches = int(np.ceil(np.sqrt(num_patches)))\n\n# plt.figure(figsize=(12, 12))\n# for i in range(num_patches):\n#     plt.subplot(sqrt_num_patches, sqrt_num_patches, i + 1)\n#     patch = last_hidden_state[0,i].reshape(patch_size, patch_size).detach().cpu().numpy()\n#     plt.imshow(patch, cmap='gray')\n#     plt.axis('off')\n#     plt.title(f'Patch {i}')\n\n# plt.suptitle('Patches Extracted by Vision Transformer', y=0.92)\n# plt.tight_layout()\n# plt.show()\n#     # Process the outputs as needed\n    # For example, extract steatosis regions and measure their area proportion\n\n","metadata":{"id":"bCzKdNRSUTwo","outputId":"85c0e747-5016-463f-9c48-51f1c7e5fbc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"a-5x4GOhZe-n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"I9f2n7k2adAT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"7ma-znbz-Iut"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install tiffslide","metadata":{"id":"QoioQ3gsJ4pt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"W3Gr2wMIH5r6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"zJACcMaqOc9T","outputId":"d08de8ba-1241-49df-ded3-a47423ea79f2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"K-B1Xgwl-vqO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"ZFojMHZ9PmJz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tifffile import TiffFile\n\ndef extract_metadata(tiff_file_path):\n    \"\"\"\n    Extract metadata from a TIFF file.\n    \"\"\"\n    with TiffFile(tiff_file_path) as tif:\n        # Access the TIFF tags (metadata)\n        metadata = tif.pages[0].tags\n        return metadata\n\nif __name__ == '__main__':\n    tiff_file_path = '/content/liver_tiff/133_4.tiff'\n    metadata = extract_metadata(tiff_file_path)\n\n    # Print the extracted metadata (you can customize this part)\n    print(\"Extracted metadata:\")\n    for tag in metadata:\n        print(f\"{tag.name}: {tag.value}\")\n","metadata":{"id":"lgJUBnMYT7hv","outputId":"a84a5b55-ec0d-42e4-9353-c72eb3b03a04"},"execution_count":null,"outputs":[]}]}